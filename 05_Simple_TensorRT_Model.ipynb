{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><a href=\"https://www.nvidia.com/en-us/deep-learning-ai/education/\"><img src=\"./assets/DLI_Header.png\"></a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying a Model for Inference at Production Scale\n",
    "\n",
    "## 05 - Simple TensorRT Model\n",
    "-------\n",
    "\n",
    "**Table of Contents**\n",
    "\n",
    "* [Introduction](#introduction)\n",
    "* [TensorRT - Programmable Inference Accelerator](#tensorrt)\n",
    "* [Create Model Directory Structure](#structure)\n",
    "* [Convert ONNX to TensorRT](#model)\n",
    "* [Create Configuration File](#configuration)\n",
    "* [Load Model in Triton Inference Server](#load)\n",
    "* [Send Inference Request to Server](#infer)\n",
    "* [Exercise](#exercise)\n",
    "* [Conclusion](#conclusion)\n",
    "\n",
    "\n",
    "<a id=\"introduction\"></a>\n",
    "### Introduction\n",
    "\n",
    "In this notebook, we will take our previously created ONNX model and convert it to a TensorRT representation to be deployed in Triton Inference Server. TensorRT is a highly optimized package that takes trained models and optimizes them for inference. We'll see how to create model directory structures and configuration files within Triton Inference Server, how to work with `trtexec`, and how to send inference requests to the models deployed within Triton Inference Server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"tensorrt\"></a>\n",
    "### TensorRT - Programmable Inference Accelerator\n",
    "\n",
    "NVIDIA TensorRT™ is a platform for high-performance deep learning inference. It includes a deep learning inference optimizer and runtime that delivers low latency and high-throughput for deep learning inference applications. TensorRT-based applications perform up to 40x faster than CPU-only platforms during inference. \n",
    "\n",
    "With TensorRT, you can optimize neural network models trained in all major frameworks, calibrate for lower precision with high accuracy, and finally deploy to hyperscale data centers, embedded, or automotive product platforms.\n",
    "\n",
    "Here are some great resources for getting started on TensorRT:\n",
    " \n",
    "* Main Page: https://developer.nvidia.com/tensorrt\n",
    "* Blogs: https://devblogs.nvidia.com/speed-up-inference-tensorrt/\n",
    "* Download: https://developer.nvidia.com/nvidia-tensorrt-download\n",
    "* Documentation: https://docs.nvidia.com/deeplearning/sdk/tensorrt-developer-guide/index.html\n",
    "* Sample Code: https://docs.nvidia.com/deeplearning/sdk/tensorrt-sample-support-guide/index.html\n",
    "* GitHub: https://github.com/NVIDIA/TensorRT\n",
    "* NGC Container: https://ngc.nvidia.com/catalog/containers/nvidia:tensorrt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"structure\"></a>\n",
    "### Create Model Directory Structure\n",
    "\n",
    "Triton Inference Server serves models within a model repository. When you first run Triton Inference Server, you'll specify the model repository where the models reside:\n",
    "\n",
    "```\n",
    "tritonserver --model-repository=/models\n",
    "```\n",
    "\n",
    "Each model resides in its own model subdirectory within the model repository - i.e. each directory within `/models` represents a unique model. For example, in this notebook we'll be deploying two TensorRT models in the `simple-tensorrt-fp32-model` and `simple-tensorrt-fp16-model`, where one model is optimized to use FP32 data types and the other is optimized to use FP16 data types. \n",
    "\n",
    "All models typically follow a similar directory structure. Within each of these directories, we'll create a configuration file `config.pbtxt` that details information about the model - e.g. batch size, input shapes, deployment backend (PyTorch, ONNX, TensorFlow, TensorRT, etc.) and more. We'll explore the configuration file later in this notebook.\n",
    "\n",
    "Additionally, we can create one or more versions of our model. Each version lives under a subdirectory names with the respective version number, starting with `1`. It is within this subdirectory where our model files reside (e.g. `model.plan`).\n",
    "\n",
    "```\n",
    "root@server:/models$ tree\n",
    ".\n",
    "└── simple-tensorrt-model\n",
    "    ├── 1\n",
    "    │   └── model.plan\n",
    "    └── config.pbtxt\n",
    "\n",
    "```\n",
    "\n",
    "We can also add a file representing the names of the outputs. We have omitted this step in this notebook for the sake of brevity. For more details on how to work with model repositories and model directory structures in Triton Inference Server, please see the documentation here: https://github.com/triton-inference-server/server/blob/r20.12/docs/model_repository.md\n",
    "\n",
    "Below, we'll create the model directory structure for each of our TensorRT models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p models/simple-tensorrt-fp32-model/\n",
    "!mkdir -p models/simple-tensorrt-fp32-model/1/\n",
    "!mkdir -p models/simple-tensorrt-fp16-model/\n",
    "!mkdir -p models/simple-tensorrt-fp16-model/1/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"model\"></a>\n",
    "### Convert ONNX to TensorRT\n",
    "\n",
    "In the previous [02_Simple_PyTorch_Model.ipynb](02_Simple_PyTorch_Model.ipynb) notebook, we created an ONNX representation of our ResNet50 model and saved it as as `model.onnx` file (If you are returning to this notebook after the time for a previous session has expired, please rerun notebook 02). In this section, we'll take that ONNX representation and convert it to a TensorRT plan using the `trtexec` command line utility. `trtexec` is easy to use and more details on installation and documentation can be found here: https://github.com/NVIDIA/TensorRT/tree/master/samples/trtexec\n",
    "\n",
    "To learn more about the `trtexec` command, uncomment and execute the cell below (warning it's quite verbose)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !trtexec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!trtexec \\\n",
    "  --onnx=models/simple-onnx-model/1/model.onnx \\\n",
    "  --explicitBatch \\\n",
    "  --optShapes=actual_input_1:16x3x224x224 \\\n",
    "  --maxShapes=actual_input_1:32x3x224x224 \\\n",
    "  --minShapes=actual_input_1:1x3x224x224 \\\n",
    "  --shapes=actual_input_1:1x3x224x224 \\\n",
    "  --saveEngine=models/simple-tensorrt-fp32-model/1/model.plan\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert our ONNX representation to a TensorRT plan, we'll point to our `model.onnx` file and specify the output for our newly created TensorRT plan. It's as simple as that!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By adding the `--fp16` flag, we can specify that our TensorRT plan will be optimized for FP16. There are a lot of benefits to using FP16, mainly of which is that it is faster (fewer computations) and uses less memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!trtexec \\\n",
    "  --onnx=models/simple-onnx-model/1/model.onnx \\\n",
    "  --explicitBatch \\\n",
    "  --optShapes=actual_input_1:16x3x224x224 \\\n",
    "  --maxShapes=actual_input_1:32x3x224x224 \\\n",
    "  --minShapes=actual_input_1:1x3x224x224 \\\n",
    "  --shapes=actual_input_1:1x3x224x224 \\\n",
    "  --saveEngine=models/simple-tensorrt-fp16-model/1/model.plan \\\n",
    "  --fp16\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"configuration\"></a>\n",
    "### Create Configuration File\n",
    "\n",
    "With our models defined and written out in TensorRT plan representations, we now turn our attention to creating configuration files for our models.\n",
    "\n",
    "A minimal model configuration must specify the name of the model, the platform and/or backend properties, the max_batch_size property, and the input and output tensors of the model (name, data type, and shape).\n",
    "\n",
    "For more details on how to create model configuration files within Triton Inference Server, please see the documentation: \n",
    "https://github.com/triton-inference-server/server/blob/r20.12/docs/model_configuration.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = \"\"\"\n",
    "name: \"simple-tensorrt-fp32-model\"\n",
    "platform: \"tensorrt_plan\"\n",
    "max_batch_size: 32\n",
    "input [\n",
    " {\n",
    "    name: \"actual_input_1\"\n",
    "    data_type: TYPE_FP32\n",
    "    format: FORMAT_NCHW\n",
    "    dims: [ 3, 224, 224 ]\n",
    "  }\n",
    "]\n",
    "output {\n",
    "    name: \"output1\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 1000 ]\n",
    "  }\n",
    "\"\"\"\n",
    "\n",
    "with open('models/simple-tensorrt-fp32-model/config.pbtxt', 'w') as file:\n",
    "    file.write(configuration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also create a configuration file for the TensorRT Fp16 model. Note that our input and output data types still remain in their FP32 representation - the internal layers and activations of our neural network will use the FP16 data type but our input and output data will still be in FP32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = \"\"\"\n",
    "name: \"simple-tensorrt-fp16-model\"\n",
    "platform: \"tensorrt_plan\"\n",
    "max_batch_size: 32\n",
    "input [\n",
    " {\n",
    "    name: \"actual_input_1\"\n",
    "    data_type: TYPE_FP32\n",
    "    format: FORMAT_NCHW\n",
    "    dims: [ 3, 224, 224 ]\n",
    "  }\n",
    "]\n",
    "output {\n",
    "    name: \"output1\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 1000 ]\n",
    "  }\n",
    "\"\"\"\n",
    "\n",
    "with open('models/simple-tensorrt-fp16-model/config.pbtxt', 'w') as file:\n",
    "    file.write(configuration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"load\"></a>\n",
    "### Load Model in Triton Inference Server\n",
    "\n",
    "\n",
    "With our model directory structures created, models defined and exported, and configuration files created, we will now wait for Triton Inference Server to load our models. We have set up this lab to use Triton Inference Server in **polling** mode. This means that Triton Inference Server will continuously poll for modifications to our models or for newly created models - once every 30 seconds. Please run the cell below to allow time for Triton Inference Server to poll for new models/modifications before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sleep 45"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, our models should be deployed and ready to use! To confirm Triton Inference Server is up and running, we can see a `curl` request to the below URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -v triton:8000/v2/health/ready"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The HTTP request returns status 200 if Triton is ready and non-200 if it is not ready.\n",
    "\n",
    "We can also send a `curl` request to our model endpoints to confirm our models are deployed and ready to use. This `curl` request returns status 200 if the model is ready and non-200 if it is not ready. \n",
    "\n",
    "Additionally, we will also see information about our models such:\n",
    "\n",
    "* The name of our model,\n",
    "* The versions available for our model,\n",
    "* The backend platform (e.g. tensorrt_plan), \n",
    "* The inputs and outputs, with their respective names, data types, and shapes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -v triton:8000/v2/models/simple-tensorrt-fp32-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -v triton:8000/v2/models/simple-tensorrt-fp16-model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"infer\"></a>\n",
    "### Send Inference Request to Server\n",
    "\n",
    "With our models deployed, it is now time to send inference requests to our models. \n",
    "\n",
    "First, we'll load the `tritonclient.http` module and a utility function for working with NumPy data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tritonclient.http as tritonhttpclient\n",
    "from tritonclient.utils import triton_to_np_dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll load our ImageNet input labels, our image of our goldfish, and some helper functions from TensorFlow Keras for loading the image, resizing the image, preprocessing the input, and decoding the predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('./imagenet-simple-labels.json') as file:\n",
    "    labels = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "img_path = './assets/goldfish.jpg'\n",
    "image_pil = Image.open(img_path)\n",
    "image_pil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "image_numpy = image.img_to_array(img)\n",
    "image_numpy = np.expand_dims(image_numpy, axis=0)\n",
    "image_numpy = preprocess_input(image_numpy)\n",
    "image_numpy = np.transpose(image_numpy, [0, 3, 1, 2]) / 255.\n",
    "print(image_numpy.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll define the input and output names of our model, the name of our model, the URL where our models are deployed with Triton Inference Server (in this case local host of `triton:8000`), and our model version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERBOSE = False\n",
    "input_name = 'actual_input_1'\n",
    "output_name = 'output1'\n",
    "model_name = 'simple-tensorrt-fp32-model'\n",
    "url = 'triton:8000'\n",
    "model_version = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll instantiate our client `triton_client` using the `tritonhttpclient.InferenceServerClient` class access the model metadata with the `.get_model_metadata()` method as well as get our model configuration with the `get_model_config()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triton_client = tritonhttpclient.InferenceServerClient(url=url, verbose=VERBOSE)\n",
    "model_metadata = triton_client.get_model_metadata(model_name=model_name, model_version=model_version)\n",
    "model_config = triton_client.get_model_config(model_name=model_name, model_version=model_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll instantiate a placeholder for our input data using the input name, shape, and data type expected. We'll set the data of the input to be the NumPy array representation of our goldfish image. We'll also instantiate a placeholder for our output data using just the output name.\n",
    "\n",
    "Lastly, we'll submit our input to the Triton Inference Server using the `triton_client.infer()` method, specifying our model name, model version, inputs, and outputs and convert our result to a NumPy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input0 = tritonhttpclient.InferInput(input_name, (1, 3, 224, 224), 'FP32')\n",
    "input0.set_data_from_numpy(image_numpy, binary_data=False)\n",
    "\n",
    "output = tritonhttpclient.InferRequestedOutput(output_name, binary_data=False)\n",
    "response = triton_client.infer(model_name, model_version=model_version, \n",
    "                               inputs=[input0], outputs=[output])\n",
    "logits = response.as_numpy('output1')\n",
    "logits = np.asarray(logits, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's all there is to it! We can identify the largest logit value and confirm that our model correctly inferred that our image is, indeed, a goldfish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labels[np.argmax(logits)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"exercise\"></a>\n",
    "### Exercise #2 - Submit an Inference Request to the TensorRT FP16 model\n",
    "\n",
    "We leave it as an exercise for the participant to submit an inference to the deployed TensorRT Fp16 model. If you get stuck (or want to confirm your answer),  click the `...` to reveal the answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Define names and shapes\n",
    "\n",
    "**Hint**: Try looking at the TensorRT Fp16 `configuration` defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERBOSE = FIXME\n",
    "input_name = FIXME\n",
    "input_shape = FIXME\n",
    "input_dtype = FIXME\n",
    "output_name = FIXME\n",
    "model_name = FIXME\n",
    "url = FIXME\n",
    "model_version = FIXME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "VERBOSE = False\n",
    "input_name = 'actual_input_1'\n",
    "input_shape = (1, 3, 224, 224)\n",
    "input_dtype = 'FP32'\n",
    "output_name = 'output1'\n",
    "model_name = 'simple-tensorrt-fp16-model'\n",
    "url = 'triton:8000'\n",
    "model_version = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Get model information from Triton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triton_client = FIXME\n",
    "model_metadata = FIXME\n",
    "model_config = FIXME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "triton_client = tritonhttpclient.InferenceServerClient(url=url, verbose=VERBOSE)\n",
    "model_metadata = triton_client.get_model_metadata(model_name=model_name, model_version=model_version)\n",
    "model_config = triton_client.get_model_config(model_name=model_name, model_version=model_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Define inputs and outputs to get an inference response from Triton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input0 = FIXME\n",
    "\n",
    "output = FIXME\n",
    "response = FIXME\n",
    "\n",
    "logits = response.as_numpy(output_name)\n",
    "logits = np.asarray(logits, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "input0 = tritonhttpclient.InferInput(input_name, input_shape, input_dtype)\n",
    "input0.set_data_from_numpy(image_numpy, binary_data=False)\n",
    "\n",
    "output = tritonhttpclient.InferRequestedOutput(output_name, binary_data=False)\n",
    "response = triton_client.infer(model_name, model_version=model_version, \n",
    "                               inputs=[input0], outputs=[output])\n",
    "logits = response.as_numpy(output_name)\n",
    "logits = np.asarray(logits, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Verify the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labels[np.argmax(logits)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"conclusion\"></a>\n",
    "### Conclusion\n",
    "\n",
    "In this notebook, we showed how to take our previously created ONNX model and convert it to a TensorRT representation to be deployed in Triton Inference Server. We also saw how to create model directory structures and configuration files within Triton Inference Server, how to work with `trtexec`, and how to send inference requests to the models deployed within Triton Inference Server.\n",
    "\n",
    "We kindly ask that you do some clean up and run the cell below. This will free up GPU memory for other section of the lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
