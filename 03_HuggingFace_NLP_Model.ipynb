{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><a href=\"https://www.nvidia.com/en-us/deep-learning-ai/education/\"><img src=\"./assets/DLI_Header.png\"></a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying a Model for Inference at Production Scale\n",
    "\n",
    "## 03 - HuggingFace Model\n",
    "-------\n",
    "\n",
    "**Table of Contents**\n",
    "\n",
    "* [Introduction](#introduction)\n",
    "* [Define a HuggingFace Pre-Trained Model](#model)\n",
    "* [Trace Model with TorchScript](#torchscript)\n",
    "* [Create Model Directory Structure](#structure)\n",
    "* [Create Configuration File](#configuration)\n",
    "* [Load Model in Triton Inference Server](#load)\n",
    "* [Send Inference Request to Server](#infer)\n",
    "* [Exercise](#exercise)\n",
    "* [Conclusion](#conclusion)\n",
    "\n",
    "\n",
    "<a id=\"introduction\"></a>\n",
    "### Introduction\n",
    "\n",
    "In this notebook, we will create a PyTorch `XLMRobertaForSequenceClassification` model from HuggingFace, write it out as a native PyTorch model using TorchScript generated code, and deploy it using Triton Inference Server. RoBERTa is an evolution of the BERT model architecture. More information about it can be found [here](https://huggingface.co/docs/transformers/model_doc/roberta). Our goal is to see how Triton can handle a more complicated model. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"structure\"></a>\n",
    "### Create Model Directory Structure\n",
    "\n",
    "Below, we'll create our model directory structure. For more details on how to create model directory structures within PyTorch, see the previous notebook [02_Simple_PyTorch_Model.ipynb](02_Simple_PyTorch_Model.ipynb).\n",
    "\n",
    "```\n",
    "root@server:/models$ tree\n",
    ".\n",
    "├── huggingface-model\n",
    "│   ├── 1\n",
    "│   │   └── model.pt\n",
    "│   └── config.pbtxt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p models/huggingface-model\n",
    "!mkdir -p models/huggingface-model/1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"model\"></a>\n",
    "### Define a HuggingFace Pre-Trained Model\n",
    "\n",
    "In this section, we'll import a text tokenization function to create tokens of our inputs for the `XLMRobertaForSequenceClassification` model. We'll wrap our model, set it to evaluation mode, and allocate it on the GPU. Lastly, we'll generate the TorchScript code using the `torch.jit.trace` function and pass in our dummy input and save the model as `model.pt` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import XLMRobertaForSequenceClassification, XLMRobertaTokenizer\n",
    "\n",
    "\n",
    "R_tokenizer = XLMRobertaTokenizer.from_pretrained('joeddav/xlm-roberta-large-xnli')\n",
    "premise = \"Jupiter's Biggest Moons Started as Tiny Grains of Hail\"\n",
    "hypothesis = 'This text is about space & cosmos'\n",
    "\n",
    "input_ids = R_tokenizer.encode(premise, hypothesis, return_tensors='pt', \n",
    "                               max_length=256, padding='max_length')\n",
    "\n",
    "mask = input_ids != 1\n",
    "mask = mask.long()\n",
    "\n",
    "\n",
    "class PyTorch_to_TorchScript(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PyTorch_to_TorchScript, self).__init__()\n",
    "        self.model = XLMRobertaForSequenceClassification.from_pretrained('joeddav/xlm-roberta-large-xnli', return_dict=False)\n",
    "    def forward(self, data, attention_mask=None):\n",
    "        return self.model(data.cuda(), attention_mask.cuda())\n",
    "\n",
    "pt_model = PyTorch_to_TorchScript().eval().cuda()\n",
    "traced_script_module = torch.jit.trace(pt_model, (input_ids, mask))\n",
    "traced_script_module.save('models/huggingface-model/1/model.pt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"configuration\"></a>\n",
    "### Create Configuration File\n",
    "\n",
    "Next, we'll create our configuration file. For more details on how to create model directory structures within PyTorch, see the previous notebook [02_Simple_PyTorch_Model.ipynb](02_Simple_PyTorch_Model.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = \"\"\"\n",
    "name: \"huggingface-model\"\n",
    "platform: \"pytorch_libtorch\"\n",
    "max_batch_size: 1024\n",
    "input [\n",
    " {\n",
    "    name: \"input__0\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [ 256 ]\n",
    "  } ,\n",
    "{\n",
    "    name: \"input__1\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [ 256 ]\n",
    "  }\n",
    "]\n",
    "output {\n",
    "    name: \"output__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 3 ]\n",
    "  }\n",
    "\"\"\"\n",
    "\n",
    "with open('models/huggingface-model/config.pbtxt', 'w') as file:\n",
    "    file.write(configuration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"load\"></a>\n",
    "### Load Model in Triton Inference Server\n",
    "\n",
    "\n",
    "With our model directory structures created, models defined and exported, and configuration files created, we will now wait for Triton Inference Server to load our models. We have set up this lab to use Triton Inference Server in **polling** mode. This means that Triton Inference Server will continuously poll for modifications to our models or for newly created models - once every 30 seconds. Please run the cell below to allow time for Triton Inference Server to poll for new models/modifications before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sleep 45"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, our models should be deployed and ready to use! To confirm Triton Inference Server is up and running, we can see a `curl` request to the below URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -v triton:8000/v2/health/ready"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The HTTP request returns status 200 if Triton is ready and non-200 if it is not ready.\n",
    "\n",
    "We can also send a `curl` request to our model endpoints to confirm our models are deployed and ready to use. This `curl` request returns status 200 if the model is ready and non-200 if it is not ready. \n",
    "\n",
    "Additionally, we will also see information about our models such:\n",
    "\n",
    "* The name of our model,\n",
    "* The versions available for our model,\n",
    "* The backend platform (e.g. pytorch_libtorch \n",
    "* The inputs and outputs, with their respective names, data types, and shapes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -v triton:8000/v2/models/huggingface-model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"infer\"></a>\n",
    "### Send Inference Request to Server\n",
    "\n",
    "With our HuggingFace model deployed, it is now time to send inference requests to our models. \n",
    "\n",
    "First, we'll do some housekeeping and restart our Jupyter notebook kernel. This will free up some of the GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll load the `tritonclient.http` module and a utility function for working with NumPy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tritonclient.http as tritonhttpclient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll define the input and output names of our model, the name of our model, the URL where our models are deployed with Triton Inference Server (in this case local host of `triton:8000`), and our model version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERBOSE = False\n",
    "input_name = ['input__0', 'input__1']\n",
    "input_dtype = 'INT32'\n",
    "output_name = 'output__0'\n",
    "model_name = 'huggingface-model'\n",
    "url = 'triton:8000'\n",
    "model_version = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll instantiate our client `triton_client` using the `tritonhttpclient.InferenceServerClient` class access the model metadata with the `.get_model_metadata()` method as well as get our model configuration with the `get_model_config()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triton_client = tritonhttpclient.InferenceServerClient(url=url, verbose=VERBOSE)\n",
    "model_metadata = triton_client.get_model_metadata(model_name=model_name, model_version=model_version)\n",
    "model_config = triton_client.get_model_config(model_name=model_name, model_version=model_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we'll create our tokenizer, apply our tokenizer to our premise and topic, and manipulate the resulting data to be passed to Triton Inference Server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import XLMRobertaTokenizer\n",
    "\n",
    "\n",
    "# instantiate our tokenizer\n",
    "R_tokenizer = XLMRobertaTokenizer.from_pretrained('joeddav/xlm-roberta-large-xnli')\n",
    "\n",
    "# create our premise and topic to be passed into the model\n",
    "premise = 'Jupiter’s Biggest Moons Started as Tiny Grains of Hail'\n",
    "topic = 'This text is about space & cosmos'\n",
    "\n",
    "# encode our inputs, convert to numpy arrays, create our mask, and do some reshaping\n",
    "input_ids = R_tokenizer.encode(premise, topic, max_length=256, truncation=True, padding='max_length')\n",
    "input_ids = np.array(input_ids, dtype=np.int32)\n",
    "mask = input_ids != 1\n",
    "mask = np.array(mask, dtype=np.int32)\n",
    "mask = mask.reshape(1, 256) \n",
    "input_ids = input_ids.reshape(1, 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll instantiate a placeholder for our input data using the input name, shape, and data type expected. We'll set the data of the input to be the NumPy array representation of our text. We'll also instantiate a placeholder for our output data using just the output name.\n",
    "\n",
    "Lastly, we'll submit our input to the Triton Inference Server using the `triton_client.infer()` method, specifying our model name, model version, inputs, and outputs and convert our result to a NumPy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input0 = tritonhttpclient.InferInput(input_name[0], (1, 256), input_dtype)\n",
    "input0.set_data_from_numpy(input_ids, binary_data=False)\n",
    "input1 = tritonhttpclient.InferInput(input_name[1], (1, 256), input_dtype)\n",
    "input1.set_data_from_numpy(mask, binary_data=False)\n",
    "output = tritonhttpclient.InferRequestedOutput(output_name,  binary_data=False)\n",
    "response = triton_client.infer(model_name, model_version=model_version, inputs=[input0, input1], outputs=[output])\n",
    "logits = response.as_numpy(output_name)\n",
    "logits = np.asarray(logits, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we post-process our data to ignore the \"neutral\" (dimension 1) result of our logits and take the probability of \"entailment\" (2) as the probability of the label being true. And that's all there is to it! Our model identifies that our premise is in fact about space and cosmos!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "\n",
    "\n",
    "entail_contradiction_logits = logits[:,[0,2]]\n",
    "probs = softmax(entail_contradiction_logits)\n",
    "true_prob = probs[:,1].item() * 100\n",
    "print(f'Probability that the label is true: {true_prob:0.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"conclusion\"></a>\n",
    "### Conclusion\n",
    "\n",
    "In this notebook, we showed how to create a PyTorch XLMRobertaForSequenceClassification model from HuggingFace, write it out as a native PyTorch model using TorchScript generated code, and deploy it using Triton Inference Server.\n",
    "\n",
    "We kindly ask that you do some clean up and run the cell below. This will free up GPU memory for other section of the lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf models/huggingface-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><a href=\"https://www.nvidia.com/en-us/deep-learning-ai/education/\"><img src=\"./assets/DLI_Header.png\"></a></div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
