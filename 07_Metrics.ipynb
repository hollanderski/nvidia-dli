{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><a href=\"https://www.nvidia.com/en-us/deep-learning-ai/education/\"><img src=\"./assets/DLI_Header.png\"></a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying a Model for Inference at Production Scale\n",
    "\n",
    "## 07 - Metrics\n",
    "-------\n",
    "\n",
    "**Table of Contents**\n",
    "\n",
    "* [Introduction](#introduction)\n",
    "* [Prometheus](#prometheus)\n",
    "* [Exercise](#exercise)\n",
    "* [Docker Compose](#docker-compose)\n",
    "* [Next Steps](#next-steps)\n",
    "\n",
    "<a id=\"introduction\"></a>\n",
    "### Introduction\n",
    "\n",
    "In this notebook, we will look at some of the server metrics Triton has automatically collected for us. Triton provides Prometheus metrics indicating GPU and request statistics. By default, these metrics are available at http://localhost:8002/metrics. The metrics are only available by accessing the endpoint, and are not pushed or published to any remote server. The metric format is plain text so they can be viewed directly, for example by running the `curl` command below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl triton:8002/metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"prometheus\"></a>\n",
    "### Prometheus\n",
    "\n",
    "For a format that's more pleasing on the eyes, Triton metrics are compatible with [Prometheus](https://prometheus.io/). Copy and paste your JupyterLab URL into the code cell below to generate a link to the Prometheus window. There will be a screen that looks like this:\n",
    "\n",
    "<div align=\"center\"><img src=\"./assets/Prom.png\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "my_url = \"FIXME\"\n",
    "prometheus_url = my_url.rsplit(\".com\", 1)[0] + \".com:9090/graph\"\n",
    "prometheus_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we're pushing our hardware too much by looking at the GPU utilization. This could help us diagnose potential crashes or we could set up an [alert](https://prometheus.io/docs/alerting/latest/overview/) if we're close to 100% and need more resources.\n",
    "\n",
    "Click the earth icon, and select `nv_gpu_utilization`. Then, click \"Execute\".\n",
    "\n",
    "<div align=\"center\"><img src=\"./assets/Prom_Add_Metric.png\"></div>\n",
    "\n",
    "This will produce a line that looks like this:\n",
    "\n",
    "*nv_gpu_utilization{gpu_uuid=\"GPU-76eed5e4-a509-ea60-8ce3-5c9b82f9252b\", instance=\"triton:8002\", job=\"prometheus\"}*\n",
    "\n",
    "Click the `Graph` tab, and a graph will appear with the GPU utilization over the past hour.\n",
    "\n",
    "<a id=\"exercise\"></a>\n",
    "### Exercise #3 - Make your own dashboard\n",
    "\n",
    "Other metrics can be compared side by side. Scroll down to the bottom of the Prometheus window and click the \"Add Panel\" button on the left. This allows us to repeat the process above. Add a few more metrics of your choosing and analyze your previous activity. Can you recognize when you sent requests to Triton from the previous notebooks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"docker-compose\"></a>\n",
    "### Docker Compose\n",
    "\n",
    "One of the most straightforward ways of setting up both Triton and Prometheus is with a tool called [Docker Compose](https://docs.docker.com/compose/). It allows us to deploy multiple [Docker containers](https://www.docker.com/resources/what-container) that can share data and other resources. We highly recommend learning [Docker basics](https://www.docker.com/101-tutorial) before proceeding.\n",
    "\n",
    "For now, let's focus on `triton` and break down each of the keys:\n",
    "* [command](https://docs.docker.com/compose/compose-file/compose-file-v2/#command): The command for the container to run once it's built. In this case, we're running the command to initiate the server if we had installed the Triton Inference Server Library locally as [described here](https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/quickstart.html#run-triton-inference-server).\n",
    "* [image](https://docs.docker.com/compose/compose-file/compose-file-v2/#image): The base image that we're building off of, in this case, the [Triton Inference Server](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/tritonserver/tags) image.\n",
    "* [shm-size](https://docs.docker.com/compose/compose-file/compose-file-v2/#shm_size): The amount of memory to share with the container. In this case, we're giving it 1 gigabyte for faster computation.\n",
    "* [ulimits](https://docs.docker.com/compose/compose-file/compose-file-v2/#ulimits): The max number of open file descriptors per process explained in this [Stack Overflow](https://stackoverflow.com/questions/24955883/what-is-the-max-opened-files-limitation-on-linux) post.\n",
    "* [ports](https://docs.docker.com/compose/compose-file/compose-file-v2/#ports): The ports to expose from the container.\n",
    "* [volumes](https://docs.docker.com/compose/compose-file/compose-file-v2/#volume-configuration-reference): A directory that can be shared between a container and it's host.\n",
    "\n",
    "Below is the a Docker compose file very similar to what we used to set up this course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "IPython.display.Code(filename=\"assets/docker-compose.yml\", language=\"yaml\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's focus on the `prometheus` service. It's simpler than the `triton` service, but we still need to provide it with a [configuration file](https://prometheus.io/docs/prometheus/latest/configuration/configuration/). Here are some of the keys.\n",
    "\n",
    "* `global`: Defines properties we want to add to every  Prometheus job.\n",
    "  * `scrape_interval`: How often a job pulls from a data source\n",
    "  * `external_labels`: The labels to add to any tine series or alert\n",
    "* [scrape_configs](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#scrape_config): Specifies a target and how to interact with it.\n",
    "\n",
    "In this case, we're telling it to pull information from `triton` (as defined in the Docker Compose file above) metrics every 5 seconds. We can give this process a `job_name` to make it easier to identify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Code(filename=\"assets/prometheus.yml\", language=\"yaml\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For additional resources on how to utilize the Triton Inference Server's metrics, please consult:\n",
    "\n",
    "* [Triton Inference Server Metrics Documentation](https://github.com/triton-inference-server/server/blob/r20.12/docs/metrics.md)\n",
    "* [Saleforce Blogpost on Benchmarking Triton Inference Server](https://blog.einstein.ai/benchmarking-tensorrt-inference-server/)\n",
    "\n",
    "\n",
    "<a id=\"next-steps\"></a>\n",
    "### Next Steps\n",
    "\n",
    "As this online environment already has a Triton Inference Server deployed, our last challenge to you is setup Triton on your own hardware. We hope the above resources can provide a starting point. For more information, please check out the [Getting Started Guide](https://github.com/triton-inference-server/server/blob/r21.11/docs/quickstart.md).\n",
    "\n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><a href=\"https://www.nvidia.com/en-us/deep-learning-ai/education/\"><img src=\"./assets/DLI_Header.png\"></a></div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
