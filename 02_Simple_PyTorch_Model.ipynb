{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><a href=\"https://www.nvidia.com/en-us/deep-learning-ai/education/\"><img src=\"./assets/DLI_Header.png\"></a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying a Model for Inference at Production Scale\n",
    "\n",
    "## 02 - Simple PyTorch Model\n",
    "-------\n",
    "\n",
    "**Table of Contents**\n",
    "\n",
    "* [Introduction](#introduction)\n",
    "* [Create Model Directory Structure](#structure)\n",
    "* [Define a Simple PyTorch Model](#model)\n",
    "* [Trace Model with TorchScript](#torchscript)\n",
    "* [Create Configuration File](#configuration)\n",
    "* [Load Model in Triton Inference Server](#load)\n",
    "* [Send Inference Request to Server](#infer)\n",
    "* [Exercise](#exercise)\n",
    "* [Conclusion](#conclusion)\n",
    "\n",
    "\n",
    "<a id=\"introduction\"></a>\n",
    "### Introduction\n",
    "\n",
    "In this notebook, we will create a PyTorch ResNet50 model, write it out as a native PyTorch model and in its ONNX representation, and deploy it using Triton Inference Server. We'll see how to create model directory structures and configuration files within Triton Inference Server, how to work with TorchScript and ONNX, and how to send inference requests to the models deployed within Triton Inference Server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"structure\"></a>\n",
    "### Create Model Directory Structure\n",
    "\n",
    "Triton Inference Server serves models within a model repository. When you first run Triton Inference Server, you'll specify the model repository where the models reside:\n",
    "\n",
    "```\n",
    "tritonserver --model-repository=/models\n",
    "```\n",
    "\n",
    "Each model resides in its own model subdirectory within the model repository - i.e. each directory within `/models` represents a unique model. For example, in this notebook we'll be deploying two models: a `simple-onnx-model` and a `simple-pytorch-model`. \n",
    "\n",
    "All models typically follow a similar directory structure. Within each of these directories, we'll create a configuration file `config.pbtxt` that details information about the model - e.g. batch size, input shapes, deployment backend (PyTorch, ONNX, TensorFlow, TensorRT, etc.) and more. We'll explore the configuration file later in this notebook.\n",
    "\n",
    "Additionally, we can create one or more versions of our model. Each version lives under a subdirectory name with the respective version number, starting with `1`. It is within this subdirectory where our model files reside (e.g. `model.onnx`, `model.pt`).\n",
    "\n",
    "```\n",
    "root@server:/models$ tree\n",
    ".\n",
    "├── simple-onnx-model\n",
    "│   ├── 1\n",
    "│   │   └── model.onnx\n",
    "│   └── config.pbtxt\n",
    "├── simple-pytorch-model\n",
    "│   ├── 1\n",
    "│   │   └── model.pt\n",
    "│   └── config.pbtxt\n",
    "\n",
    "```\n",
    "\n",
    "We can also add a file representing the names of the outputs. We have omitted this step in this notebook for the sake of brevity. For more details on how to work with model repositories and model directory structures in Triton Inference Server, please see the documentation here: https://github.com/triton-inference-server/server/blob/r20.12/docs/model_repository.md\n",
    "\n",
    "Below, we'll create the model directory structure for each of our PyTorch and ONNX models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p models/simple-pytorch-model\n",
    "!mkdir -p models/simple-pytorch-model/1\n",
    "!mkdir -p models/simple-onnx-model\n",
    "!mkdir -p models/simple-onnx-model/1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"model\"></a>\n",
    "### Define a Simple PyTorch Model\n",
    "\n",
    "In this next section, we'll define a simple PyTorch ResNet50 model. We'll specify that we will use a pretrained model, which will instantiate the ResNet50 model with the weights learned from training on ImageNet. After defining our `Model` class, we will instantiate this model, set the model to evaluation mode with the `.eval()` method, and allocate the model on the GPU with the `.cuda()` method. Learn more about how to train PyTorch models on GPUs with CUDA with [this article](https://medium.com/ai%C2%B3-theory-practice-business/use-gpu-in-your-pytorch-code-676a67faed09)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import models\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.model = models.resnet50(pretrained=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "model = Model().eval().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll load the ImageNet labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('./imagenet-simple-labels.json') as file:\n",
    "    labels = json.load(file)\n",
    "\n",
    "print(labels[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before working with Triton Inference Server, let's confirm our ResNet50 model pre-trained on ImageNet works on a sample image. We'll use an image of goldfish - feel free to try this with your own images!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "image = Image.open('./assets/goldfish.jpg')\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we'll create a transformation pipeline to take an image, resize it to `(256, 256)`, take a center crop resulting in an image of size `(224, 224)`, convert it to a PyTorch Tensor, and then normalize the image using the means and standard deviations of the ImageNet dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "imagenet_mean = [0.485, 0.456, 0.406]\n",
    "imagenet_std = [0.485, 0.456, 0.406]\n",
    "\n",
    "resize = transforms.Resize((256, 256))\n",
    "center_crop = transforms.CenterCrop(224)\n",
    "to_tensor = transforms.ToTensor()\n",
    "normalize = transforms.Normalize(mean=imagenet_mean,\n",
    "                                 std=imagenet_std)\n",
    "\n",
    "transform = transforms.Compose([resize, center_crop, to_tensor, normalize])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we'll apply our transformation pipeline to our image, add a dimension for our batch sizes with the `.unsqueeze(0)` method, and allocate our image on the GPU with the `.cuda()` method. We'll pass our image through our model to get the `logits`.\n",
    "\n",
    "After moving the `logits` to CPU, we'll then use the `torch.topk` function to access the values and indices of the top 3 `logits`. We see our top result is indeed a goldfish. Awesome!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_tensor = transform(image).unsqueeze(0).cuda()\n",
    "logits = model(image_tensor)\n",
    "\n",
    "K = 3\n",
    "values, indices = torch.topk(logits, K)\n",
    "\n",
    "values = values.detach().tolist()[0]\n",
    "indices = indices.detach().tolist()[0]\n",
    "\n",
    "for i in range(K):\n",
    "    print(values[i], indices[i], labels[indices[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"torchscript\"></a>\n",
    "### Trace Model with TorchScript\n",
    "\n",
    "\n",
    "We have defined our model and confirmed it works as expected. Before writing out our model as a `model.pt` file, we will trace our model using TorchScript. TorchScript is a way to create serializable and optimizable models from PyTorch code. Any TorchScript program can be saved from a Python process and loaded in a process where there is no Python dependency. This is how we'll load our PyTorch model into Triton Inference Server (which uses the `libtorch` backend).\n",
    "\n",
    "There are two ways to generate a model with TorchScript - using either the `torch.jit.script` function or the `torch.jit.trace` function. \n",
    "\n",
    "Using `torch.jit.script` on a function or `nn.Module` will inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return a `ScriptModule` or `ScriptFunction`.\n",
    "\n",
    "Using `torch.jit.trace` on a function will return an executable or `ScriptFunction` that will be optimized using just-in-time compilation.\n",
    "\n",
    "It may not be immediately clear whether to use `torch.jit.script` or `torch.jit.trace`. Typically, `torch.jit.script` is more flexible and allows you work with different batch sizes while `torch.jit.trace` requires you to pass in an example dummy input with a fixed batch size. In general, I recommend starting with `torch.jit.script`.\n",
    "\n",
    "For more details on TorchScript, please see:\n",
    "\n",
    "* The TorchScript documentation: https://pytorch.org/docs/stable/jit.html\n",
    "* This very insightful blogpost: https://paulbridger.com/posts/mastering-torchscript/\n",
    "\n",
    "Below, we'll define a wrapper around our model, set our model wrapper to evaluation mode, and allocate our model on the GPU. Next, we'll generate our TorchScript code with the `torch.jit.script` function and write out our model as `model.pt` in the version `1` subdirectory of our `simple-pytorch-model` model directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PyTorch_to_TorchScript(nn.Module):\n",
    "    def __init__(self, my_model):\n",
    "        super(PyTorch_to_TorchScript, self).__init__()\n",
    "        self.model = my_model.model\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "torchscript_model = PyTorch_to_TorchScript(model).eval().cuda()\n",
    "traced_script_module = torch.jit.script(torchscript_model)\n",
    "traced_script_module.save('models/simple-pytorch-model/1/model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also convert our model to an ONNX representation. Open Neural Network Exchange (ONNX) is an open ecosystem that empowers AI developers to choose the right tools as their project evolves. ONNX provides an open source format for AI models, both deep learning and traditional ML. It defines an extensible computation graph model, as well as definitions of built-in operators and standard data types. Currently we focus on the capabilities needed for inferencing (scoring).\n",
    "\n",
    "Below, we'll create a Torch Tensor of random data in the shape of our input images and allocate it on the GPU. We'll also specify the input and output names of the model. We'll see in the next section where these values are used in our configuration model.\n",
    "\n",
    "Lastly, we'll export our model in an ONNX representation as a `model.onnx` file in the version `1` subdirectory of our `simple-onnx-model` model directory, specifying the dummy input and the appropriate input and output names. We'll also pass in a dictionary mapping the input and outname names to which dimensions should be the batch size. This allows us to work with variable batch sizes - without using the `dynamic_axes` parameter, our ONNX model would be hard coded to use whichever batch size we chose for our dummy input, which is in this case, is batch size 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = torch.randn(1, 3, 224, 224).cuda()\n",
    "\n",
    "input_names = ['actual_input_1'] + ['learned_%d' % i for i in range(16)]\n",
    "output_names = ['output1']\n",
    "\n",
    "torch.onnx.export(model, dummy_input, \n",
    "                  'models/simple-onnx-model/1/model.onnx', verbose=False, \n",
    "                  input_names=input_names, output_names=output_names, \n",
    "                  dynamic_axes={'actual_input_1': {0: 'batch_size'}, 'output1': {0: 'batch_size'}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"configuration\"></a>\n",
    "### Create Configuration File\n",
    "\n",
    "With our models defined and written out in TorchScript and ONNX representations, we now turn our attention to creating configuration files for our models.\n",
    "\n",
    "A minimal model configuration must specify the name of the model, the platform and/or backend properties, the max_batch_size property, and the input and output tensors of the model (name, data type, and shape).\n",
    "\n",
    "\n",
    "For more details on how to create model configuration files within Triton Inference Server, please see the documentation: \n",
    "https://github.com/triton-inference-server/server/blob/r20.12/docs/model_configuration.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = \"\"\"\n",
    "name: \"simple-pytorch-model\"\n",
    "platform: \"pytorch_libtorch\"\n",
    "max_batch_size: 32\n",
    "input [\n",
    " {\n",
    "    name: \"input__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    format: FORMAT_NCHW\n",
    "    dims: [ 3, 224, 224 ]\n",
    "  }\n",
    "]\n",
    "output {\n",
    "    name: \"output__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 1000 ]\n",
    "  }\n",
    "\"\"\"\n",
    "\n",
    "with open('models/simple-pytorch-model/config.pbtxt', 'w') as file:\n",
    "    file.write(configuration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also create a configuration file for the ONNX model. Note that the name attribute of our input and output tensors are different, since we specified the input and output names when we exported the ONNX model. Please note that the `platform` has been updated to `onnxruntime_onnx`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = \"\"\"\n",
    "name: \"simple-onnx-model\"\n",
    "platform: \"onnxruntime_onnx\"\n",
    "max_batch_size: 32\n",
    "input [\n",
    " {\n",
    "    name: \"actual_input_1\"\n",
    "    data_type: TYPE_FP32\n",
    "    format: FORMAT_NCHW\n",
    "    dims: [ 3, 224, 224 ]\n",
    "  }\n",
    "]\n",
    "output {\n",
    "    name: \"output1\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 1000]\n",
    "  }\n",
    "\"\"\"\n",
    "\n",
    "with open('models/simple-onnx-model/config.pbtxt', 'w') as file:\n",
    "    file.write(configuration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"load\"></a>\n",
    "### Load Model in Triton Inference Server\n",
    "\n",
    "\n",
    "With our model directory structures created, models defined and exported, and configuration files created, we will now wait for Triton Inference Server to load our models. We have set up this lab to use Triton Inference Server in **polling** mode. This means that Triton Inference Server will continuously poll for modifications to our models or for newly created models - once every 30 seconds. Please run the cell below to allow time for Triton Inference Server to poll for new models/modifications before proceeding. Due to the asynchronous nature of this step, we have added 15 seconds to be safe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sleep 45"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, our models should be deployed and ready to use! To confirm Triton Inference Server is up and running, we can see a `curl` request to the below URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -v triton:8000/v2/health/ready"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The HTTP request returns status 200 if Triton is ready and non-200 if it is not ready.\n",
    "\n",
    "We can also send a `curl` request to our model endpoints to confirm our models are deployed and ready to use. This `curl` request returns status 200 if the model is ready and non-200 if it is not ready. \n",
    "\n",
    "Additionally, we will also see information about our models such:\n",
    "\n",
    "* The name of our model,\n",
    "* The versions available for our model,\n",
    "* The backend platform (e.g. pytorch_libtorch, onnxruntime_onnx), \n",
    "* The inputs and outputs, with their respective names, data types, and shapes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -v triton:8000/v2/models/simple-pytorch-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -v triton:8000/v2/models/simple-onnx-model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"infer\"></a>\n",
    "### Send Inference Request to Server\n",
    "\n",
    "With our models deployed, it is now time to send inference requests to our models. \n",
    "\n",
    "First, we'll load the `tritonclient.http` module and a utility function for working with NumPy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tritonclient.http as tritonhttpclient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll define the input and output names of our model, the name of our model, the URL where our models are deployed with Triton Inference Server (in this case the host `triton:8000`), and our model version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERBOSE = False\n",
    "input_name = 'input__0'\n",
    "input_shape = (1, 3, 224, 224)\n",
    "input_dtype = 'FP32'\n",
    "output_name = 'output__0'\n",
    "model_name = 'simple-pytorch-model'\n",
    "url = 'triton:8000'\n",
    "model_version = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll instantiate our client `triton_client` using the `tritonhttpclient.InferenceServerClient` class access the model metadata with the `.get_model_metadata()` method as well as get our model configuration with the `get_model_config()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triton_client = tritonhttpclient.InferenceServerClient(url=url, verbose=VERBOSE)\n",
    "model_metadata = triton_client.get_model_metadata(model_name=model_name, model_version=model_version)\n",
    "model_config = triton_client.get_model_config(model_name=model_name, model_version=model_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll convert our previouly defined image of our goldfish (currently as a Torch Tensor) to a NumPy array on the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_numpy = image_tensor.cpu().numpy()\n",
    "print(image_numpy.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll instantiate a placeholder for our input data using the input name, shape, and data type expected. We'll set the data of the input to be the NumPy array representation of our goldfish image. We'll also instantiate a placeholder for our output data using just the output name.\n",
    "\n",
    "Lastly, we'll submit our input to the Triton Inference Server using the `triton_client.infer()` method, specifying our model name, model version, inputs, and outputs and convert our result to a NumPy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input0 = tritonhttpclient.InferInput(input_name, input_shape, input_dtype)\n",
    "input0.set_data_from_numpy(image_numpy, binary_data=False)\n",
    "\n",
    "output = tritonhttpclient.InferRequestedOutput(output_name, binary_data=False)\n",
    "response = triton_client.infer(model_name, model_version=model_version, \n",
    "                               inputs=[input0], outputs=[output])\n",
    "logits = response.as_numpy(output_name)\n",
    "logits = np.asarray(logits, dtype=np.float32)\n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's all there is to it! We can identify the largest logit value and confirm that our model correctly inferred that our image is, indeed, a goldfish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labels[np.argmax(logits)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"exercise\"></a>\n",
    "### Exercise #1 - Submit an Inference Request to the ONNX model\n",
    "\n",
    "We leave it as an exercise for the participant to submit an inference to the deployed ONNX model. If you get stuck (or want to confirm your answer), click the `...` to reveal the answer.\n",
    "\n",
    "Hint: Just copying the inference code from above won't work - pay attention to model name and the input and output names in the configuration file we defined for ONNX."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Define names and shapes\n",
    "\n",
    "**Hint**: Try looking at the ONNX `configuration` defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERBOSE = FIXME\n",
    "input_name = FIXME\n",
    "input_shape = FIXME\n",
    "input_dtype = FIXME\n",
    "output_name = FIXME\n",
    "model_name = FIXME\n",
    "url = FIXME\n",
    "model_version = FIXME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "VERBOSE = False\n",
    "input_name = 'actual_input_1'\n",
    "input_shape = (1, 3, 224, 224)\n",
    "input_dtype = 'FP32'\n",
    "output_name = 'output1'\n",
    "model_name = 'simple-onnx-model'\n",
    "url = 'triton:8000'\n",
    "model_version = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Get model information from Triton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triton_client = tritonhttpclient.FIXME(url=url, verbose=VERBOSE)\n",
    "model_metadata = triton_client.FIXME(model_name=model_name, model_version=model_version)\n",
    "model_config = triton_client.FIXME(model_name=model_name, model_version=model_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "triton_client = tritonhttpclient.InferenceServerClient(url=url, verbose=VERBOSE)\n",
    "model_metadata = triton_client.get_model_metadata(model_name=model_name, model_version=model_version)\n",
    "model_config = triton_client.get_model_config(model_name=model_name, model_version=model_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Test an image\n",
    "\n",
    "No `FIXME`s here, view image shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_numpy = image_tensor.cpu().numpy()\n",
    "print(image_numpy.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Define inputs and outputs to get an inference response from Triton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input0 = FIXME\n",
    "\n",
    "output = FIXME\n",
    "response = FIXME\n",
    "\n",
    "logits = response.as_numpy(output_name)\n",
    "logits = np.asarray(logits, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "input0 = tritonhttpclient.InferInput(input_name, input_shape, input_dtype)\n",
    "input0.set_data_from_numpy(image_numpy, binary_data=False)\n",
    "\n",
    "output = tritonhttpclient.InferRequestedOutput(output_name, binary_data=False)\n",
    "response = triton_client.infer(model_name, model_version=model_version, \n",
    "                               inputs=[input0], outputs=[output])\n",
    "logits = response.as_numpy(output_name)\n",
    "logits = np.asarray(logits, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Verify the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labels[np.argmax(logits)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"conclusion\"></a>\n",
    "### Conclusion\n",
    "\n",
    "In this notebook, we showed how to create a PyTorch ResNet50 model, write it out as a native PyTorch model and in its ONNX representation, and deploy it using Triton Inference Server. We saw how to create model directory structures and configuration files within Triton Inference Server, how to work with TorchScript and ONNX, and how to send inference requests to the models deployed within Triton Inference Server.\n",
    "\n",
    "We kindly ask that you do some clean up and run the cell below. This will free up GPU memory for other section of the lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><a href=\"https://www.nvidia.com/en-us/deep-learning-ai/education/\"><img src=\"./assets/DLI_Header.png\"></a></div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
