{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><a href=\"https://www.nvidia.com/en-us/deep-learning-ai/education/\"><img src=\"./assets/DLI_Header.png\"></a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying a Model for Inference at Production Scale\n",
    "\n",
    "## 06 - Advanced Inference\n",
    "-------\n",
    "\n",
    "**Table of Contents**\n",
    "\n",
    "* [Introduction](#introduction)\n",
    "* [Housekeeping](#housekeeping)\n",
    "* [Performance Analyzer](#performance)\n",
    "* [Model Analyzer](#model)\n",
    "* [CPU Benchmark](#cpu)\n",
    "* [Variable Batch Size](#variable)\n",
    "* [Dynamic Batching](#dynamic-batching)\n",
    "* [HTTP vs. gRPC](#protocol)\n",
    "* [Asynchronous Inference](#async)\n",
    "* [Shared Memory](#shared)\n",
    "* [Conclusion](#conclusion)\n",
    "\n",
    "<a id=\"introduction\"></a>\n",
    "### Introduction\n",
    "\n",
    "In this notebook, we will explore how to do advanced inferencing with Triton Inference Server. We will explore tools like the Performance Analyzer, the Model Analyzer, how to access metrics, and how to optimize latency and throughput in your applications using the GPU, variable batch size, dynamic batching, different protocols like HTTP and gRPC, asynchronous inference, and shared memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"housekeeping\"></a>\n",
    "### Housekeeping\n",
    "\n",
    "Before we go any further, we'll do some housekeeping and import some of the client libraries we'll be using as well as define some variables we'll use throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import tritonclient.http as tritonhttpclient\n",
    "import tritonclient.grpc as tritongrpcclient\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "http_url = 'triton:8000'\n",
    "grpc_url = 'triton:8001'\n",
    "verbose = False\n",
    "concurrency = 32\n",
    "model_version = '1'\n",
    "triton_http_client = tritonhttpclient.InferenceServerClient(url=http_url, verbose=verbose, concurrency=concurrency)\n",
    "triton_grpc_client = tritongrpcclient.InferenceServerClient(url=grpc_url, verbose=verbose)\n",
    "input_dtype = 'FP32'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"performance\"></a>\n",
    "### Performance Analyzer\n",
    "\n",
    "A critical part of optimizing the inference performance of your model is being able to measure changes in performance as you experiment with different optimization strategies. The `perf_analyzer` application (previously known as `perf_client`) performs this task for the Triton Inference Server. The `perf_analyzer` is included with the client examples which are available from several sources.\n",
    "\n",
    "The `perf_analyzer` application generates inference requests to your model and measures the throughput and latency of those requests. To get representative results, perf_analyzer measures the throughput and latency over a time window, and then repeats the measurements until it gets stable values. By default `perf_analyzer` uses average latency to determine stability but you can use the `--percentile` flag to stabilize results based on that confidence level. For example, if `--percentile=95` is used the results will be stabilized using the 95-th percentile request latency. \n",
    "\n",
    "For example, we can run any of the following to analyze the performance of our models:\n",
    "\n",
    "```\n",
    "perf_analyzer \\\n",
    "  -m simple-tensorflow-model \\\n",
    "  -b 1 \\\n",
    "  --concurrency-range 1:1 \\\n",
    "  --shape input_0:1,224,224,3\n",
    "\n",
    "perf_analyzer \\\n",
    "  -m simple-pytorch-model \\\n",
    "  -b 1 \\\n",
    "  --concurrency-range 1:1\n",
    "\n",
    "perf_analyzer \\\n",
    "  -m simple-onnx-model \\\n",
    "  -b 1 \\\n",
    "  --concurrency-range 1:1\n",
    "  \n",
    "perf_analyzer \\\n",
    "  -m simple-tensorrt-fp32-model \\\n",
    "  -b 1 \\\n",
    "  --concurrency-range 1:1\n",
    "  \n",
    "perf_analyzer \\\n",
    "  -m simple-tensorrt-fp16-model \\\n",
    "  -b 1 \\\n",
    "  --concurrency-range 1:1\n",
    "```\n",
    "\n",
    "Unfortunately, we're not able to run `perf_analyzer` while Triton Inference Server is deployed in **polling** mode. However, for more details on `perf_analyzer`, you can find the documentation here: https://github.com/triton-inference-server/server/blob/r20.12/docs/perf_analyzer.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"model\"></a>\n",
    "### Model Analyzer\n",
    "\n",
    "The Triton Model Analyzer is a tool that uses Performance Analyzer to send requests to your model while measuring GPU memory and compute utilization. The Model Analyzer is specifically useful for characterizing the GPU memory requirements for your model under different batching and model instance configurations. Once you have this GPU memory usage information you can more intelligently decide on how to combine multiple models on the same GPU while remaining within the memory capacity of the GPU.\n",
    "\n",
    "For more information see the [Model Analyzer repository](https://github.com/triton-inference-server/model_analyzer) and the detailed explanation provided in [Maximizing Deep Learning Inference Performance with NVIDIA Model Analyzer](https://developer.nvidia.com/blog/maximizing-deep-learning-inference-performance-with-nvidia-model-analyzer)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"cpu\"></a>\n",
    "### CPU Benchmark\n",
    "\n",
    "Before we get into some of our advanced inferencing techniques, let's first benchmark one of our models on the CPU. Triton Inference Server works not only with any kind of deep learning framework, but is also flexible enough to be able to deploy models onto the CPU. To deploy the CPU, simply add:\n",
    "\n",
    "```\n",
    "instance_group [\n",
    "    {\n",
    "      count: 1\n",
    "      kind: KIND_CPU\n",
    "    }\n",
    "  ]\n",
    "```\n",
    "\n",
    "to your configuration file. Below, we copy our `simple-pytorch-model` into a new model directory and modify the model configuration file so that Triton Inference Server will deploy it on the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf models/simple-pytorch-model-cpu/\n",
    "!cp -R models/simple-pytorch-model/ models/simple-pytorch-model-cpu/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = \"\"\"\n",
    "name: \"simple-pytorch-model-cpu\"\n",
    "platform: \"pytorch_libtorch\"\n",
    "max_batch_size: 32\n",
    "instance_group [\n",
    "    {\n",
    "      count: 1\n",
    "      kind: KIND_CPU\n",
    "    }\n",
    "  ]\n",
    "input [\n",
    " {\n",
    "    name: \"input__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    format: FORMAT_NCHW\n",
    "    dims: [ 3, 224, 224 ]\n",
    "  }\n",
    "]\n",
    "output {\n",
    "    name: \"output__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 1000 ]\n",
    "  }\n",
    "\"\"\"\n",
    "\n",
    "with open('models/simple-pytorch-model-cpu/config.pbtxt', 'w') as file:\n",
    "    file.write(configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sleep 45"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll go through our usual process of defining our `InferInput` and `InferRequestedOutput` objects and assign data to our inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_name = 'input__0'\n",
    "input_shape = (1, 3, 224, 224)\n",
    "output_name = 'output__0'\n",
    "model_name = 'simple-pytorch-model'\n",
    "\n",
    "input0 = tritonhttpclient.InferInput(input_name, input_shape, input_dtype)\n",
    "dummy_data = np.ones(shape=input_shape, dtype=np.float32)\n",
    "input0.set_data_from_numpy(dummy_data, binary_data=True)\n",
    "\n",
    "output = tritonhttpclient.InferRequestedOutput(output_name, binary_data=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's submit 1000 requests (each request is batch size 1) to our `simple-pytorch-model`, deployed on the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: batch size 1\n",
    "\n",
    "start_time = time.time()\n",
    "requests = []\n",
    "request_count = 1000\n",
    "for i in tqdm(range(request_count)):\n",
    "    requests.append(triton_http_client.infer(model_name, model_version=model_version, \n",
    "                                             inputs=[input0], outputs=[output]))\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "print('Average Latency: ~{} seconds'.format((end_time - start_time) / request_count))\n",
    "print('Average Throughput: ~{} examples / second'.format(batch_size * request_count / (end_time - start_time)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll submit the same 1000 requests to our `simple-pytorch-model-cpu`, deployed on the CPU. The difference is quite stark!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'simple-pytorch-model-cpu'\n",
    "\n",
    "# note: feel free to stop running this cell at any time!\n",
    "\n",
    "start_time = time.time()\n",
    "requests = []\n",
    "request_count = 1000\n",
    "for i in tqdm(range(request_count)):\n",
    "    requests.append(triton_http_client.infer(model_name, model_version=model_version, \n",
    "                                             inputs=[input0], outputs=[output]))\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Average Latency: ~{} seconds'.format((end_time - start_time) / request_count))\n",
    "print('Average Throughput: ~{} examples / second'.format(batch_size * request_count / (end_time - start_time)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"variable\"></a>\n",
    "### Variable Batch Size\n",
    "\n",
    "Until now, we're worked with data inputs that have a batch size of 1. However, we might often want to send different batch sizes such as 4, 8, 32, or even higher. This has a natural tradeoff of latency and throughput. Since our batches are larger, it might take longer to process an individual batch - increasing the latency. However, since the GPU has more data to work with and we're less constrained by networking and I/O, we might see an increase in throughput - or the number of examples that can be processed per second. \n",
    "\n",
    "Below, we'll use our `simple-tensorrt-fp16-model` and pass in 10000 requests of batch size 1. We see this process takes ~45 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_name = 'actual_input_1'\n",
    "input_shape = (1, 3, 224, 224)\n",
    "output_name = 'output1'\n",
    "model_name = 'simple-tensorrt-fp16-model'\n",
    "\n",
    "input0 = tritonhttpclient.InferInput(input_name, input_shape, input_dtype)\n",
    "dummy_data = np.ones(shape=input_shape, dtype=np.float32)\n",
    "input0.set_data_from_numpy(dummy_data, binary_data=True)\n",
    "output = tritonhttpclient.InferRequestedOutput(output_name, binary_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: batch size 1\n",
    "\n",
    "start_time = time.time()\n",
    "requests = []\n",
    "request_count = 10000\n",
    "for i in tqdm(range(request_count)):\n",
    "    requests.append(triton_http_client.infer(model_name, model_version=model_version, \n",
    "                                             inputs=[input0], outputs=[output]))\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "print('Average Latency: ~{} seconds'.format((end_time - start_time) / request_count))\n",
    "print('Average Throughput: ~{} examples / second'.format(batch_size * request_count / (end_time - start_time)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll use a batch size of 32 and pass in 300 requests. We see how by increasing the batch size, we increase our average latency but are able to increase the total throughput."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (32, 3, 224, 224)\n",
    "input0 = tritonhttpclient.InferInput(input_name, input_shape, input_dtype)\n",
    "dummy_data = np.ones(shape=input_shape, dtype=np.float32)\n",
    "input0.set_data_from_numpy(dummy_data, binary_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: batch size 32\n",
    "\n",
    "start_time = time.time()\n",
    "requests = []\n",
    "request_count = 300\n",
    "for i in tqdm(range(request_count)):\n",
    "    requests.append(triton_http_client.infer(model_name, model_version=model_version, \n",
    "                                             inputs=[input0], outputs=[output]))\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "print('Average Latency: ~{} seconds'.format((end_time - start_time) / request_count))\n",
    "print('Average Throughput: ~{} examples / second'.format(batch_size * request_count / (end_time - start_time)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"dynamic-batching\"></a>\n",
    "### Dynamic Batching\n",
    "\n",
    "Dynamic batching is a feature of Triton that allows inference requests to be combined by the server, so that a batch is created dynamically. Creating a batch of requests typically results in increased throughput. To enable dynamic batching, simply add:\n",
    "\n",
    "```\n",
    "dynamic_batching {\n",
    "    preferred_batch_size: [ 4, 8 ]\n",
    "    max_queue_delay_microseconds: 100\n",
    "  }\n",
    "```\n",
    "\n",
    "to your configuration file. The `preferred_batch_size property` indicates the batch sizes that the dynamic batcher should attempt to create. For example, the above configuration enables dynamic batching with preferred batch sizes of 4 and 8.\n",
    "\n",
    "The dynamic batcher can be configured to allow requests to be delayed for a limited time in the scheduler to allow other requests to join the dynamic batch. For example, the following configuration sets the maximum delay time of 100 microseconds for a request.\n",
    "\n",
    "The `max_queue_delay_microseconds` property setting changes the dynamic batcher behavior when a batch of a preferred size cannot be created. When a batch of a preferred size cannot be created from the available requests, the dynamic batcher will delay sending the batch as long as no request is delayed longer than the configured `max_queue_delay_microseconds` value. If a new request arrives during this delay and allows the dynamic batcher to form a batch of a preferred batch size, then that batch is sent immediately for inferencing. If the delay expires the dynamic batcher sends the batch as is, even though it is not a preferred size.\n",
    "\n",
    "Below, we copy our `simple-tensorrt-fp16-model` into a new model directory and modify the model configuration file so that Triton Inference Server will deploy it using dynamic batching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf models/dynamic-batching-model/\n",
    "!cp -R models/simple-tensorrt-fp16-model/ models/dynamic-batching-model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = \"\"\"\n",
    "name: \"dynamic-batching-model\"\n",
    "platform: \"tensorrt_plan\"\n",
    "dynamic_batching { \n",
    "  preferred_batch_size: [ 4, 8, 16, 32 ] \n",
    "  max_queue_delay_microseconds: 100 }\n",
    "max_batch_size: 32\n",
    "input [\n",
    " {\n",
    "    name: \"actual_input_1\"\n",
    "    data_type: TYPE_FP32\n",
    "    format: FORMAT_NCHW\n",
    "    dims: [ 3, 224, 224 ]\n",
    "  }\n",
    "]\n",
    "output {\n",
    "    name: \"output1\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 1000 ]\n",
    "  }\n",
    "\"\"\"\n",
    "\n",
    "with open('models/dynamic-batching-model/config.pbtxt', 'w') as file:\n",
    "    file.write(configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sleep 45"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we'll use our `dynamic-batching-model` and pass in 10000 requests of batch size 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (1, 3, 224, 224)\n",
    "model_name = 'dynamic-batching-model'\n",
    "\n",
    "input0 = tritonhttpclient.InferInput(input_name, input_shape, input_dtype)\n",
    "dummy_data = np.ones(shape=input_shape, dtype=np.float32)\n",
    "input0.set_data_from_numpy(dummy_data, binary_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: batch size 1\n",
    "\n",
    "start_time = time.time()\n",
    "requests = []\n",
    "request_count = 10000\n",
    "for i in tqdm(range(request_count)):\n",
    "    requests.append(triton_http_client.infer(model_name, model_version=model_version, \n",
    "                                             inputs=[input0], outputs=[output]))\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "print('Average Latency: ~{} seconds'.format((end_time - start_time) / request_count))\n",
    "print('Average Throughput: ~{} examples / second'.format(batch_size * request_count / (end_time - start_time)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"protocol\"></a>\n",
    "### HTTP vs. gRPC\n",
    "\n",
    "Clients can communicate with Triton using either an HTTP/REST or GRPC protocol, or by a C API. Most people are familiar with HTTP, which is the backbone of the internet. gRPC is a newer, open source remote procedure call system initially developed at Google in 2015 that uses HTTP/2 for transport and Protocol Buffers as the interface description language. It is highly efficient and using it is very easy. \n",
    "\n",
    "Below, we use the `tritonclient.grpc` module to instantiate new `InferInput` and `InferRequestedOutput` objects, and our `tritonclient.grpc.InferenceServerClient` instance to send 10000 requests of batch size 1 to our `dynamic-batching-model`. We can immediately see that just using a slightly different protocol can have an enormous impact on latency and throughput!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (1, 3, 224, 224)\n",
    "model_name = 'dynamic-batching-model'\n",
    "\n",
    "input0 = tritongrpcclient.InferInput(input_name, input_shape, input_dtype)\n",
    "dummy_data = np.ones(shape=input_shape, dtype=np.float32)\n",
    "input0.set_data_from_numpy(dummy_data)\n",
    "output = tritongrpcclient.InferRequestedOutput(output_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "requests = []\n",
    "request_count = 10000\n",
    "for i in tqdm(range(request_count)):\n",
    "    requests.append(triton_grpc_client.infer(model_name, model_version=model_version, \n",
    "                                             inputs=[input0], outputs=[output]))\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Average Latency: ~{} seconds'.format((end_time - start_time) / request_count))\n",
    "print('Average Throughput: ~{} examples / second'.format(batch_size * request_count / (end_time - start_time)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"async\"></a>\n",
    "### Asynchronous Inference\n",
    "\n",
    "So far, our requests have been submitted to Triton Inference Server synchronously. In other words, we submit a request to Triton, Triton then computes and returns the result, and we submit our next request. However, what if we could submit as many requests as possible, allow Triton to queue requests it hasn't yet processed, and return results as soon as they are computed? This paradigm is known as asynchronous inferencing and can result in some of dramatic speedups for throughput.\n",
    "\n",
    "Below, we create a utility `callback` function for handling asynchronous requests and submit 10000 requests of batch size 1 to our `dynamic-batching-model` using the `async_infer` method of our `tritonclient.grpc.InferenceServerClient` instance. Our improvement in throughput is incredible!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "\n",
    "results = []\n",
    "\n",
    "def callback(user_data, result, error):\n",
    "    if error:\n",
    "        user_data.append(error)\n",
    "    else:\n",
    "        user_data.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "async_requests = []\n",
    "request_count = 10000\n",
    "for i in tqdm(range(request_count)):\n",
    "    # Asynchronous inference call.\n",
    "    async_requests.append(triton_grpc_client.async_infer(model_name=model_name, inputs=[input0], \n",
    "                                                         callback=partial(callback, results), \n",
    "                                                         outputs=[output]))\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Average Latency: ~{} seconds'.format((end_time - start_time) / request_count))\n",
    "print('Average Throughput: ~{} examples / second'.format(batch_size * request_count / (end_time - start_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Example shape of one example of our output data:', results[0].as_numpy(output_name).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"shared\"></a>\n",
    "### Shared Memory\n",
    "\n",
    "Using system shared memory and CUDA shared memory to communicate tensors between the client library and Triton can significantly improve performance in some cases. Unfortunately, this area is beyond the scope of this lab but those curious are highly encouraged to check out the documentation and client examples found here: https://github.com/triton-inference-server/server/blob/r20.12/docs/client_examples.md#system-shared-memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"conclusion\"></a>\n",
    "### Conclusion\n",
    "\n",
    "In this notebook, we explored how to do advanced inferencing with Triton Inference Server. We explored tools like the Performance Analyzer, the Model Analyzer, how to access metrics, and how to optimize latency and throughput in your applications using the GPU, variable batch size, dynamic batching, different protocols like HTTP and gRPC, asynchronous inference, and shared memory.\n",
    "\n",
    "We kindly ask to do some clean up and run the cell below. This will free up GPU memory for other section of the lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><a href=\"https://www.nvidia.com/en-us/deep-learning-ai/education/\"><img src=\"./assets/DLI_Header.png\"></a></div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
