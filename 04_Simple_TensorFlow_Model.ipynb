{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><a href=\"https://www.nvidia.com/en-us/deep-learning-ai/education/\"><img src=\"./assets/DLI_Header.png\"></a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying a Model for Inference at Production Scale\n",
    "\n",
    "## 04 - Simple TensorFlow Model\n",
    "-------\n",
    "\n",
    "**Table of Contents**\n",
    "\n",
    "* [Introduction](#introduction)\n",
    "* [Create Model Directory Structure](#structure)\n",
    "* [Define a Simple TensorFlow Model](#model)\n",
    "* [Create Configuration File](#configuration)\n",
    "* [Load Model in Triton Inference Server](#load)\n",
    "* [Send Inference Request to Server](#infer)\n",
    "* [Exercise](#exercise)\n",
    "* [Conclusion](#conclusion)\n",
    "\n",
    "\n",
    "<a id=\"introduction\"></a>\n",
    "### Introduction\n",
    "\n",
    "In this notebook, we will create a TensorFlow ResNet 50 model, write it out as a `SavedModel` representation and deploy it using Triton Inference Server. We'll see how to create model directory structures and configuration files within Triton Inference Server, how to work with TensorFlow, and how to send inference requests to the models deployed within Triton Inference Server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"structure\"></a>\n",
    "### Create Model Directory Structure\n",
    "\n",
    "Triton Inference Server serves models within a model repository. When you first run Triton Inference Server, you'll specify the model repository where the models reside:\n",
    "\n",
    "```\n",
    "tritonserver --model-repository=/models\n",
    "```\n",
    "\n",
    "Each model resides in its own model subdirectory within the model repository - i.e. each directory within `/models` represents a unique model. For example, in this notebook we'll be deploying a TensorFlow model (`simple-TensorFlow-model`).\n",
    "\n",
    "All models typically follow a similar directory structure. Within each of these directories, we'll create a configuration file `config.pbtxt` that details information about the model - e.g. batch size, input shapes, deployment backend (PyTorch, ONNX, TensorFlow, TensorRT, etc.) and more. We'll explore the configuration file later in this notebook.\n",
    "\n",
    "Additionally, we can create one or more versions of our model. Each version lives under a subdirectory names with the respective version number, starting with `1`. It is within this subdirectory where our model files reside (e.g. `model.onnx`, `model.savedmodel`).\n",
    "\n",
    "```\n",
    "root@server:/models$ tree\n",
    ".\n",
    "├── simple-tensorflow-model\n",
    "│   ├── 1\n",
    "│   │   └── model.savedmodel\n",
    "│   │       ├── assets\n",
    "│   │       ├── saved_model.pb\n",
    "│   │       └── variables\n",
    "│   │           ├── variables.data-00000-of-00001\n",
    "│   │           └── variables.index\n",
    "\n",
    "```\n",
    "\n",
    "We can also add a file representing the names of the outputs. We have omitted this step in this notebook for the sake of brevity. For more details on how to work with model repositories and model directory structures in Triton Inference Server, please see the documentation here: https://github.com/triton-inference-server/server/blob/r20.12/docs/model_repository.md\n",
    "\n",
    "Below, we'll create the model directory structure for each of our TensorFlow model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p models/simple-tensorflow-model/\n",
    "!mkdir -p models/simple-tensorflow-model/1/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"model\"></a>\n",
    "### Define a Simple TensorFlow Model\n",
    "\n",
    "In this next section, we'll define a simple TensorFlow ResNet50 model. We'll specify that we will use a pretrained model, which will instantiate the ResNet50 model with the weights learned from training on ImageNet. After defining our `WrappedModel` class, we will instantiate this model, use the private `__call__` method to get the call signature, and use that to save the model using the `tf.saved_model.save` function. \n",
    "\n",
    "We'll export our model in an `SavedModel` representation in the version `1` subdirectory of our `simple-tensorflow-model` model directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.optimizer.set_jit(True)\n",
    "\n",
    "\n",
    "class WrappedModel(tf.Module):\n",
    "    def __init__(self):\n",
    "        super(WrappedModel, self).__init__()\n",
    "        self.model = tf.keras.applications.ResNet50()\n",
    "    @tf.function\n",
    "    def __call__(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "model = WrappedModel()\n",
    "call = model.__call__.get_concrete_function(tf.TensorSpec([None, None, None, None], \n",
    "                                            tf.float32, name='input_0'))\n",
    "tf.saved_model.save(model, \n",
    "                    'models/simple-tensorflow-model/1/model.savedmodel', \n",
    "                    signatures=call)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll load the ImageNet labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('./imagenet-simple-labels.json') as file:\n",
    "    labels = json.load(file)\n",
    "\n",
    "print(labels[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before working with Triton Inference Server, let's confirm our ResNet50 model pre-trained on ImageNet works on a sample image. We'll use an image of goldfish - feel free to try this with your own images!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "img_path = './assets/goldfish.jpg'\n",
    "image_pil = Image.open(img_path)\n",
    "image_pil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll import a base ResNet50 model from TensorFlow Keras as well as some helper functions for loading the image, resizing the image, preprocessing the input, and decoding the predictions. We see that our model does exactly as we expect by correctly classifying our image of a goldfish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "import numpy as np\n",
    "\n",
    "model = ResNet50(weights='imagenet')\n",
    "\n",
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "image_numpy = image.img_to_array(img)\n",
    "image_numpy = np.expand_dims(image_numpy, axis=0)\n",
    "image_numpy = preprocess_input(image_numpy)\n",
    "\n",
    "preds = model.predict(image_numpy)\n",
    "# decode the results into a list of tuples (class, description, probability)\n",
    "# (one such list for each sample in the batch)\n",
    "print('Predicted:', decode_predictions(preds, top=3)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"configuration\"></a>\n",
    "### Create Configuration File\n",
    "\n",
    "With our model defined and written out in `SavedModel` representation, we now turn our attention to creating configuration files for our models.\n",
    "\n",
    "A minimal model configuration must specify the name of the model, the platform and/or backend properties, the max_batch_size property, and the input and output tensors of the model (name, data type, and shape).\n",
    "\n",
    "\n",
    "For more details on how to create model configuration files within Triton Inference Server, please see the documentation: \n",
    "https://github.com/triton-inference-server/server/blob/r20.12/docs/model_configuration.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = \"\"\"\n",
    "name: \"simple-tensorflow-model\"\n",
    "platform: \"tensorflow_savedmodel\"\n",
    "max_batch_size: 32\n",
    "input [\n",
    " {\n",
    "    name: \"input_0\"\n",
    "    data_type: TYPE_FP32\n",
    "    format: FORMAT_NHWC\n",
    "    dims: [ 224, 224, 3 ]\n",
    "  }\n",
    "]\n",
    "output {\n",
    "    name: \"output_0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 1000 ]\n",
    "  }\n",
    "\"\"\"\n",
    "\n",
    "with open('models/simple-tensorflow-model/config.pbtxt', 'w') as file:\n",
    "    file.write(configuration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"load\"></a>\n",
    "### Load Model in Triton Inference Server\n",
    "\n",
    "\n",
    "With our model directory structures created, models defined and exported, and configuration files created, we will now wait for Triton Inference Server to load our models. We have set up this lab to use Triton Inference Server in **polling** mode. This means that Triton Inference Server will continuously poll for modifications to our models or for newly created models - once every 30 seconds. Please run the cell below to allow time for Triton Inference Server to poll for new models/modifications before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sleep 45"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, our models should be deployed and ready to use! To confirm Triton Inference Server is up and running, we can see a `curl` request to the below URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -v triton:8000/v2/health/ready"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The HTTP request returns status 200 if Triton is ready and non-200 if it is not ready.\n",
    "\n",
    "We can also send a `curl` request to our model endpoints to confirm our models are deployed and ready to use. This `curl` request returns status 200 if the model is ready and non-200 if it is not ready. \n",
    "\n",
    "Additionally, we will also see information about our models such:\n",
    "\n",
    "* The name of our model,\n",
    "* The versions available for our model,\n",
    "* The backend platform (e.g. tensorflow_savedmodel), \n",
    "* The inputs and outputs, with their respective names, data types, and shapes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -v triton:8000/v2/models/simple-tensorflow-model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"infer\"></a>\n",
    "### Send Inference Request to Server\n",
    "\n",
    "With our models deployed, it is now time to send inference requests to our models. \n",
    "\n",
    "First, we'll load the `tritonclient.http` module and a utility function for working with NumPy data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tritonclient.http as tritonhttpclient\n",
    "from tritonclient.utils import triton_to_np_dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll define the input and output names of our model, the name of our model, the URL where our models are deployed with Triton Inference Server (in this case local host of `triton:8000`), and our model version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERBOSE = False\n",
    "input_name = 'input_0'\n",
    "input_shape = (1, 224, 224, 3)\n",
    "input_dtype = 'FP32'\n",
    "output_name = 'output_0'\n",
    "model_name = 'simple-tensorflow-model'\n",
    "url = 'triton:8000'\n",
    "model_version = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll instantiate our client `triton_client` using the `tritonhttpclient.InferenceServerClient` class access the model metadata with the `.get_model_metadata()` method as well as get our model configuration with the `get_model_config()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triton_client = tritonhttpclient.InferenceServerClient(url=url, verbose=VERBOSE)\n",
    "model_metadata = triton_client.get_model_metadata(model_name=model_name, model_version=model_version)\n",
    "model_config = triton_client.get_model_config(model_name=model_name, model_version=model_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll instantiate a placeholder for our input data using the input name, shape, and data type expected. We'll set the data of the input to be the NumPy array representation of our goldfish image. We'll also instantiate a placeholder for our output data using just the output name.\n",
    "\n",
    "Lastly, we'll submit our input to the Triton Inference Server using the `triton_client.infer()` method, specifying our model name, model version, inputs, and outputs and convert our result to a NumPy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input0 = tritonhttpclient.InferInput(input_name, input_shape, input_dtype)\n",
    "input0.set_data_from_numpy(image_numpy, binary_data=False)\n",
    "\n",
    "output = tritonhttpclient.InferRequestedOutput(output_name, binary_data=False)\n",
    "response = triton_client.infer(model_name, model_version=model_version, \n",
    "                               inputs=[input0], outputs=[output])\n",
    "logits = response.as_numpy(output_name)\n",
    "logits = np.asarray(logits, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's all there is to it! We can identify the largest logit value and confirm that our model correctly inferred that our image is, indeed, a goldfish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labels[np.argmax(logits)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"conclusion\"></a>\n",
    "### Conclusion\n",
    "\n",
    "In this notebook, we showed how to create a TensorFlow ResNet50 model, write it out as a `SavedModel` representation and deploy it using Triton Inference Server. We saw how to create model directory structures and configuration files within Triton Inference Server, how to work with TensorFlow, and how to send inference requests to the models deployed within Triton Inference Server.\n",
    "\n",
    "We kindly ask that you do some clean up and run the cell below. This will free up GPU memory for other section of the lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf models/simple-tensorflow-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><a href=\"https://www.nvidia.com/en-us/deep-learning-ai/education/\"><img src=\"./assets/DLI_Header.png\"></a></div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
